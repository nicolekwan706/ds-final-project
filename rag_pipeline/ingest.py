# -*- coding: utf-8 -*-
"""ingest

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YpwQqVWfrFNk1kdLc0s8KheX3n0xTMtM
"""


from pathlib import Path
from typing import List, Dict
import json

import pandas as pd
from llama_index.core.schema import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from pypdf import PdfReader

#paths
CSV_PATH = Path("etl_cleaned_dataset.csv")
PDF_PATH = Path("The_determinants_of_box_office_performance_in_the_.pdf")
OUTPUT_JSONL = Path("ingested_documents.jsonl")

#chunking config
#about 2,000 characters ≈ 512-1024 tokens (3–4 chars/token), with overlap for continuity
CHUNK_SIZE_CHARS = 2000
CHUNK_OVERLAP_CHARS = 200

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=CHUNK_SIZE_CHARS,
    chunk_overlap=CHUNK_OVERLAP_CHARS,
)


def _safe_get(row: pd.Series, col: str) -> str:
    if col not in row or pd.isna(row[col]):
        return ""
    return str(row[col])


def build_text(row: pd.Series) -> str:
    #convert a CSV row into a readable text description of the movie
    lines: List[str] = []

    title = _safe_get(row, "title")
    year = _safe_get(row, "release_year")
    genres = _safe_get(row, "genres") or _safe_get(row, "genre")
    rating = _safe_get(row, "rating") or _safe_get(row, "rated")
    description = _safe_get(row, "description") or _safe_get(row, "plot")

    if title:
        lines.append(f"Title: {title}")
    if year:
        lines.append(f"Release year: {year}")
    if genres:
        lines.append(f"Genres: {genres}")
    if rating:
        lines.append(f"Rating: {rating}")
    if description:
        lines.append(f"Description: {description}")

    language = _safe_get(row, "language") or _safe_get(row, "original_language")
    country = _safe_get(row, "country") or _safe_get(row, "production_countries")
    imdb_rating = _safe_get(row, "imdbrating")
    imdb_votes = _safe_get(row, "imdbvotes")

    if language:
        lines.append(f"Language: {language}")
    if country:
        lines.append(f"Production countries: {country}")
    if imdb_rating:
        lines.append(f"IMDb rating: {imdb_rating}")
    if imdb_votes:
        lines.append(f"IMDb votes: {imdb_votes}")

    return "\n".join(lines).strip()


def build_metadata(row: pd.Series) -> Dict:
    def to_int_or_none(val):
        if pd.isna(val):
            return None
        try:
            return int(val)
        except Exception:
            return None

    return {
        "film_id": to_int_or_none(row.get("film_id")),
        "title": row.get("title"),
        "release_year": row.get("release_year"),
        "genres": row.get("genres") or row.get("genre"),
        "rating": row.get("rating") or row.get("rated"),
    }


def ingest_etl_dataset() -> None:
    if not CSV_PATH.exists():
        raise FileNotFoundError(f"CSV not found at {CSV_PATH.resolve()}")

    print(f"[INGEST] Loading dataset from {CSV_PATH.resolve()} ...")
    df = pd.read_csv(CSV_PATH)
    print(f"[INGEST] Rows in dataset: {len(df)}")

    documents: List[Document] = []
    total_chunks = 0

    #ingest CSV rows
    for _, row in df.iterrows():
        full_text = build_text(row)
        if not full_text:
            continue

        base_metadata = build_metadata(row)

        chunks = text_splitter.split_text(full_text)

        for i, chunk in enumerate(chunks):
            metadata = {
                **base_metadata,
                "chunk_id": i,
                "source": "csv",
            }
            doc = Document(text=chunk, metadata=metadata)
            documents.append(doc)
            total_chunks += 1

    print(f"[INGEST] After CSV, total chunks: {total_chunks}")

    #ingest PDF (box office determinants paper)
    if PDF_PATH.exists():
        print(f"[INGEST] Loading PDF from {PDF_PATH.resolve()} ...")
        reader = PdfReader(str(PDF_PATH))

        for page_num, page in enumerate(reader.pages):
            page_text = page.extract_text() or ""
            page_text = page_text.strip()
            if not page_text:
                continue

            pdf_chunks = text_splitter.split_text(page_text)
            for i, chunk in enumerate(pdf_chunks):
                metadata = {
                    "source": "pdf",
                    "file_name": PDF_PATH.name,
                    "page": page_num,
                    "chunk_id": i,
                    "title": "The determinants of box office performance in the film industry revisited",
                }
                doc = Document(text=chunk, metadata=metadata)
                documents.append(doc)
                total_chunks += 1

        print(f"[INGEST] After PDF, total chunks: {total_chunks}")
    else:
        print(f"[INGEST] PDF not found at {PDF_PATH.resolve()}, skipping PDF ingestion.")

    #write out JSONL
    with OUTPUT_JSONL.open("w", encoding="utf-8") as f_out:
        for doc in documents:
            record = {
                "text": doc.text,
                "metadata": doc.metadata,
            }
            f_out.write(json.dumps(record, ensure_ascii=False) + "\n")

    print(f"[INGEST] Wrote {total_chunks} records to {OUTPUT_JSONL.resolve()}")


if __name__ == "__main__":
    ingest_etl_dataset()
